{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import multivariate_normal\n",
    "import os\n",
    "\n",
    "\n",
    "size=[]\n",
    "weights=[]\n",
    "cov=[]\n",
    "means=[]\n",
    "\n",
    "def gauss(X, mean_vector, covariance_matrix):\n",
    "    if (np.abs(np.linalg.det(covariance_matrix))==0):\n",
    "        print(\"ERROR\")\n",
    "     # a= (2*np.pi)**(-len(X)/2)*np.abs(np.prod((np.linalg.eigvals(covariance_matrix))))**(-1/2)*np.exp(-np.dot(np.dot((X-mean_vector).T, np.linalg.pinv(covariance_matrix)), (X-mean_vector))/2)\n",
    "    b= (2*np.pi)**(-len(X)/2)*(np.linalg.det(covariance_matrix))**(-1/2)*np.exp(-np.dot(np.dot((X-mean_vector).T, np.linalg.inv(covariance_matrix)), (X-mean_vector))/2)\n",
    "     # c= ((1/(((2*math.pi)**(X.shape[0]/2))*((np.linalg.det(covariance_matrix))**0.5)))*math.exp(-0.5*np.matmul(np.matmul((X-mean_vector).T,np.linalg.pinv(covariance_matrix)),(X-mean_vector))))\n",
    "     # return (2*np.pi)**(-len(X)/2)*np.linalg.det(covariance_matrix)**(-1/2)*np.exp(-np.dot(np.dot((X-mean_vector).T, np.linalg.inv(covariance_matrix)), (X-mean_vector))/2)\n",
    "\n",
    "    return b\n",
    "\n",
    "# The only hyperparameter is k ( no.of components for each class)\n",
    "k=2\n",
    "train=['coast','forest','opencountry','street','tallbuilding']\n",
    "\n",
    "for c, train_file in enumerate(train):\n",
    "    arr = os.listdir('./'+train_file+'/train')\n",
    "    data=pd.DataFrame()\n",
    "\n",
    "    for i in range(len(arr)):\n",
    "        data_2=pd.read_csv(train_file+'/train/'+arr[i],header=None,delim_whitespace=True)\n",
    "        #print(data.shape)\n",
    "        #coast_train.concat(data)\n",
    "        data=pd.concat([data,data_2],ignore_index=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #data=pd.read_csv('dataset/'+train_file+'/train.csv')\n",
    "    data=data.to_numpy()\n",
    "    X=data\n",
    "    size.append(len(X))\n",
    "    print(f\"\\n\\n\\nClass {c}\")\n",
    "#     print(size)\n",
    "    kmeans=KMeans(n_clusters=k,random_state=0).fit(X)\n",
    "    # kmeans=KMeans(n_clusters=k).fit(X)\n",
    "    means_old=kmeans.cluster_centers_\n",
    "    labels=kmeans.labels_\n",
    "\n",
    "\n",
    "    N=len(X)\n",
    "    r_old=np.zeros((len(X),k)) # form a Z ( indicator ) matrix\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        r_old[i,labels[i]]=1\n",
    "\n",
    "    Nq_old=np.sum(r_old,axis=0) # sum conatins the number of elements belonging\n",
    "                                 # to each cluster\n",
    "\n",
    "    print(\"\\nOriginal effective number of elements in each cluster\")\n",
    "    print(Nq_old)\n",
    "    # Initialization\n",
    "\n",
    "    #cov2 is a 3-d array containing the covariance matrix of each cluster\n",
    "    cov_old=np.zeros([k,X.shape[1],X.shape[1]])\n",
    "    Wq_old =np.zeros([k,1]) ## weight of each cluster\n",
    "\n",
    "    for i in range(k):\n",
    "        Nq=Nq_old[i]\n",
    "        Wq_old[i]= Nq/N\n",
    "        tp=np.zeros([X.shape[1],X.shape[1]])\n",
    "\n",
    "        for p in range(X.shape[0]):\n",
    "            le=X[p,:]-means_old[i]\n",
    "            le=np.reshape(le,[le.shape[0],1])\n",
    "            tp=tp+r_old[p,i]*(np.dot(le,le.T))\n",
    "        tp=tp/Nq\n",
    "\n",
    "#         d= np.diag(tp)\n",
    "#         tp=np.diag(d)\n",
    "        cov_old[i,:,:]=tp.copy()\n",
    "\n",
    "    ll_old= 0.0\n",
    "    for n in range(len(X)):\n",
    "        ll_old = ll_old + np.log(sum([Wq_old[j]*multivariate_normal.pdf(X[n], means_old[j], cov_old[j],allow_singular=True) for j in range(k)]))\n",
    "\n",
    "    print(f\"\\nInitial log-likehood = {ll_old}\")\n",
    "\n",
    "    convergence=False\n",
    "    iter_convergence=0\n",
    "    run=0\n",
    "    runs=1000\n",
    "    epsilon=100\n",
    "\n",
    "    while (convergence == False and run<runs):\n",
    "\n",
    "        # ''' --------------------------   E - STEP   -------------------------- '''\n",
    "\n",
    "        # Initiating the r matrix, every row contains the probabilities\n",
    "        # for every cluster for this row\n",
    "\n",
    "        r_new = np.zeros((len(X), k))  # responsibilty matrix\n",
    "\n",
    "        # Calculating the r matrix\n",
    "        for n in range(len(X)):\n",
    "            for i in range(k):\n",
    "                r_new[n][i] = Wq_old[i] * multivariate_normal.pdf(X[n], means_old[i], cov_old[i],allow_singular=True)\n",
    "                r_new[n][i] /= sum([Wq_old[j]*multivariate_normal.pdf(X[n], means_old[j], cov_old[j],allow_singular=True) for j in range(k)])\n",
    "\n",
    "        # Calculating the N effective elemts fro each component\n",
    "        Nq_new = np.sum(r_new, axis=0)\n",
    "\n",
    "\n",
    "        # ''' --------------------------   M - STEP   -------------------------- '''\n",
    "\n",
    "\n",
    "        # Updating the weights list\n",
    "        Wq_new =np.zeros([k,1]) ## weight of each cluster\n",
    "        for i in range(k):\n",
    "            Wq_new[i]= Nq_new[i]/ N\n",
    "\n",
    "\n",
    "        # Initializing the mean vector as a zero vector\n",
    "        means_new = np.zeros((k, len(X[0])))\n",
    "\n",
    "        # Updating the mean vector\n",
    "        for i in range(k):\n",
    "            for n in range(len(X)):\n",
    "                means_new[i] = means_new[i] + r_new[n][i] * X[n]\n",
    "            means_new[i] = means_new [i]/Nq_new[i]\n",
    "\n",
    "\n",
    "\n",
    "        # Initiating the list of the covariance matrixes\n",
    "        cov_new =np.zeros([k,X.shape[1],X.shape[1]])\n",
    "\n",
    "        # Updating the covariance matrices\n",
    "        for i in range(k):\n",
    "            Nq=Nq_new[i]\n",
    "            tp=np.zeros([X.shape[1],X.shape[1]])\n",
    "\n",
    "            for p in range(X.shape[0]):\n",
    "                le=X[p,:]-means_new[i]\n",
    "                le=np.reshape(le,[le.shape[0],1])\n",
    "                tp=tp+r_new[p,i]*(np.dot(le,le.T))\n",
    "\n",
    "            tp=tp/Nq\n",
    "#             d= np.diag(tp)\n",
    "#             tp=np.diag(d)\n",
    "            cov_new[i,:,:]=tp.copy()\n",
    "\n",
    "\n",
    "        # print(f\"\\nRun= {run}\\n\")\n",
    "#         print(np.sum(Nq_new))\n",
    "#         print(\"\\nWeights\\n\")\n",
    "#         print(np.sum(Wq_new))\n",
    "#         print(Wq_new)\n",
    "#         print(np.sum(r_new))\n",
    "#         print(\"\\n------------------\")\n",
    "\n",
    "        # Calculating log-likelhood\n",
    "        ll_new=0\n",
    "        for n in range(len(X)):\n",
    "            ll_new = ll_new + np.log(sum([Wq_new[j]*multivariate_normal.pdf(X[n], means_new[j], cov_new[j],allow_singular=True) for j in range(k)]))\n",
    "\n",
    "    #     print(ll_new)\n",
    "        diff=ll_new-ll_old\n",
    "\n",
    "    #     print(diff)\n",
    "\n",
    "        #Convergence condition\n",
    "        if diff < 1e-3:\n",
    "            iter_convergence=run\n",
    "            convergence=True\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            ll_old=ll_new.copy()\n",
    "            Wq_old= Wq_new.copy()\n",
    "            means_old=means_new.copy()\n",
    "            cov_old=cov_new.copy()\n",
    "\n",
    "        run= run +1\n",
    "\n",
    "    if convergence==True and run!=runs:\n",
    "        print(\"Iterations for convergence=\",iter_convergence)\n",
    "    else:\n",
    "        print(\"Estimate has not converged yet, more runs needed\")\n",
    "    print(f\"Final log-likehood = {ll_new}\")\n",
    "\n",
    "    print(\"\\nEffective number of elements in each cluster is\")\n",
    "    print(Nq_new)\n",
    "#     ass=np.sum(Nq_new)\n",
    "#     print(ass)\n",
    "    weights.append(Wq_new)\n",
    "    means.append(means_new)\n",
    "    cov.append(cov_new)\n",
    "\n",
    "print(\"\\n##############################################################################\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on validation set using full covariance matrix and k=2 is 70.6896551724138\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "size=np.array(size)\n",
    "prior_class=size/np.sum(size)\n",
    "\n",
    "validation_set=['coast','forest','opencountry','street','tallbuilding']\n",
    "# validation_set=['train_1.csv','train_2.csv','train_3.csv','train_4.csv','train_5.csv']\n",
    "\n",
    "\n",
    "real=[]\n",
    "predicted=[]\n",
    "\n",
    "for c, train_file in enumerate(validation_set):\n",
    "    arr = os.listdir('./'+train_file+'/dev')\n",
    "    #data=pd.DataFrame()\n",
    "\n",
    "    for i in range(int(len(arr)/2)):\n",
    "        data_2=pd.read_csv(train_file+'/dev/'+arr[i],header=None,delim_whitespace=True)\n",
    "        #print(data.shape)\n",
    "        #coast_train.concat(data)\n",
    "        #print(data_2)\n",
    "        data=data_2.to_numpy()\n",
    "        #print(data.shape)\n",
    "        real.append(c)\n",
    "        ll_n=[]\n",
    "        for i in range(len(validation_set)):\n",
    "            ll=0\n",
    "            for p in range(data.shape[0]):\n",
    "                ll+= np.log(sum([weights[i][j]*multivariate_normal.pdf(data[p], means[i][j], cov[i][j],allow_singular=True)  for j in range(k)])) \n",
    "            ll_n.append(ll+np.log(prior_class[i]))\n",
    "        ll_n=np.array(ll_n)\n",
    "        predicted.append(np.argmax(ll_n))\n",
    "    \n",
    "#print(valid_data)\n",
    "#print(test_data)  \n",
    " \n",
    "#print(len(valid_data))\n",
    "print(\"accuracy on validation set using full covariance matrix and k=\"+str(k)+ \" is \" +str(accuracy_score(real,predicted)*100))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-b24f6f214a90>:28: RuntimeWarning: divide by zero encountered in log\n",
      "  ll+= np.log(sum([weights[i][j]*multivariate_normal.pdf(data[p], means[i][j], cov[i][j],allow_singular=True)  for j in range(k)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on Training set using full covariance matrix and k=2 is 71.72131147540983\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "size=np.array(size)\n",
    "prior_class=size/np.sum(size)\n",
    "\n",
    "validation_set=['coast','forest','opencountry','street','tallbuilding']\n",
    "# validation_set=['train_1.csv','train_2.csv','train_3.csv','train_4.csv','train_5.csv']\n",
    "\n",
    "\n",
    "real=[]\n",
    "predicted=[]\n",
    "\n",
    "for c, train_file in enumerate(validation_set):\n",
    "    arr = os.listdir('./'+train_file+'/train')\n",
    "    #data=pd.DataFrame()\n",
    "\n",
    "    for i in range(int(len(arr))):\n",
    "        data_2=pd.read_csv(train_file+'/train/'+arr[i],header=None,delim_whitespace=True)\n",
    "        #print(data.shape)\n",
    "        #coast_train.concat(data)\n",
    "        #print(data_2)\n",
    "        data=data_2.to_numpy()\n",
    "        #print(data.shape)\n",
    "        real.append(c)\n",
    "        ll_n=[]\n",
    "        for i in range(len(validation_set)):\n",
    "            ll=0\n",
    "            for p in range(data.shape[0]):\n",
    "                ll+= np.log(sum([weights[i][j]*multivariate_normal.pdf(data[p], means[i][j], cov[i][j],allow_singular=True)  for j in range(k)])) \n",
    "            ll_n.append(ll+np.log(prior_class[i]))\n",
    "        ll_n=np.array(ll_n)\n",
    "        predicted.append(np.argmax(ll_n))\n",
    "    \n",
    "#print(valid_data)\n",
    "#print(test_data)  \n",
    " \n",
    "#print(len(valid_data))\n",
    "print(\"accuracy on Training set using full covariance matrix and k=\"+str(k)+ \" is \" +str(accuracy_score(real,predicted)*100))\n",
    "\n",
    "if k==2:\n",
    "    confuse=confusion_matrix(real,predicted)\n",
    "\n",
    "    sn.heatmap(confuse/np.sum(confuse,axis=0), annot=True,\n",
    "        fmt='.2%', cmap='Blues',cbar=False,xticklabels=validation_set,yticklabels=validation_set)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel(\"Actual Class\")\n",
    "    plt.title('Confusion Matrix for GMM with full covariance matrix on Training data with k='+str(k))\n",
    "    #plt.savefig('Confusion_train_2.png')\n",
    "    #plt.xaxis.set_ticklabels(validation_set); \n",
    "    #ax.yaxis.set_ticklabels(validation_set[::-1]);\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set using full covariance matrix and k=2 is 65.3409090909091\n"
     ]
    }
   ],
   "source": [
    "if k==2:\n",
    "    size=np.array(size)\n",
    "    prior_class=size/np.sum(size)\n",
    "\n",
    "    validation_set=['coast','forest','opencountry','street','tallbuilding']\n",
    "    # validation_set=['train_1.csv','train_2.csv','train_3.csv','train_4.csv','train_5.csv']\n",
    "\n",
    "\n",
    "    real=[]\n",
    "    predicted=[]\n",
    "\n",
    "    for c, train_file in enumerate(validation_set):\n",
    "        arr = os.listdir('./'+train_file+'/dev')\n",
    "        #data=pd.DataFrame()\n",
    "\n",
    "        for i in range(int(len(arr)/2),len(arr)):\n",
    "            data_2=pd.read_csv(train_file+'/dev/'+arr[i],header=None,delim_whitespace=True)\n",
    "            #print(data.shape)\n",
    "            #coast_train.concat(data)\n",
    "            #print(data_2)\n",
    "            data=data_2.to_numpy()\n",
    "            #print(data.shape)\n",
    "            real.append(c)\n",
    "            ll_n=[]\n",
    "            for i in range(len(validation_set)):\n",
    "                ll=0\n",
    "                for p in range(data.shape[0]):\n",
    "                    ll+= np.log(sum([weights[i][j]*multivariate_normal.pdf(data[p], means[i][j], cov[i][j],allow_singular=True)  for j in range(k)])) \n",
    "                ll_n.append(ll+np.log(prior_class[i]))\n",
    "            ll_n=np.array(ll_n)\n",
    "            predicted.append(np.argmax(ll_n))\n",
    "\n",
    "    #print(valid_data)\n",
    "    #print(test_data)  \n",
    "\n",
    "    #print(len(valid_data))\n",
    "    print(\"accuracy on test set using full covariance matrix and k=\"+str(k)+ \" is \" +str(accuracy_score(real,predicted)*100))\n",
    "\n",
    "    confuse=confusion_matrix(real,predicted)\n",
    "\n",
    "    sn.heatmap(confuse/np.sum(confuse,axis=0), annot=True,\n",
    "        fmt='.2%', cmap='Blues',cbar=False,xticklabels=validation_set,yticklabels=validation_set)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel(\"Actual Class\")\n",
    "    plt.title('Confusion Matrix for GMM with full covariance matrix on Testing data with k='+str(k))\n",
    "    #plt.savefig('Confusion_train_2.png')\n",
    "    #plt.xaxis.set_ticklabels(validation_set); \n",
    "    #ax.yaxis.set_ticklabels(validation_set[::-1]);\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
