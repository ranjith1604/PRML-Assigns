{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Class 0\n",
      "\n",
      "Original effective number of elements in each cluster\n",
      "[107.  88.  56.]\n",
      "\n",
      "Initial log-likehood = [6292.33397387]\n",
      "Iterations for convergence= 14\n",
      "Final log-likehood = [6473.1081942]\n",
      "\n",
      "Effective number of elements in each cluster is\n",
      "[96.01512101 91.78637783 63.19850117]\n",
      "\n",
      "\n",
      "\n",
      "Class 1\n",
      "\n",
      "Original effective number of elements in each cluster\n",
      "[72. 97. 60.]\n",
      "\n",
      "Initial log-likehood = [9176.39023618]\n",
      "Iterations for convergence= 26\n",
      "Final log-likehood = [9418.93006826]\n",
      "\n",
      "Effective number of elements in each cluster is\n",
      "[71.0511302  75.81482239 82.13404741]\n",
      "\n",
      "\n",
      "\n",
      "Class 2\n",
      "\n",
      "Original effective number of elements in each cluster\n",
      "[ 88.  97. 102.]\n",
      "\n",
      "Initial log-likehood = [7314.44371518]\n",
      "Iterations for convergence= 25\n",
      "Final log-likehood = [7525.9882791]\n",
      "\n",
      "Effective number of elements in each cluster is\n",
      "[ 96.20345373 100.45246845  90.34407782]\n",
      "\n",
      "\n",
      "\n",
      "Class 3\n",
      "\n",
      "Original effective number of elements in each cluster\n",
      "[75. 86. 43.]\n",
      "\n",
      "Initial log-likehood = [7853.03451778]\n",
      "Iterations for convergence= 6\n",
      "Final log-likehood = [7904.37594683]\n",
      "\n",
      "Effective number of elements in each cluster is\n",
      "[72.10947993 89.88461139 42.00590868]\n",
      "\n",
      "\n",
      "\n",
      "Class 4\n",
      "\n",
      "Original effective number of elements in each cluster\n",
      "[95. 61. 93.]\n",
      "\n",
      "Initial log-likehood = [7443.54946238]\n",
      "Iterations for convergence= 24\n",
      "Final log-likehood = [7752.86861527]\n",
      "\n",
      "Effective number of elements in each cluster is\n",
      "[72.12520355 80.97095978 95.90383667]\n",
      "\n",
      "##############################################################################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "size=[]\n",
    "weights=[]\n",
    "cov=[]\n",
    "means=[]\n",
    "\n",
    "def gauss(X, mean_vector, covariance_matrix):\n",
    "    if (np.abs(np.linalg.det(covariance_matrix))==0):\n",
    "        print(\"ERROR\")\n",
    "     # a= (2*np.pi)**(-len(X)/2)*np.abs(np.prod((np.linalg.eigvals(covariance_matrix))))**(-1/2)*np.exp(-np.dot(np.dot((X-mean_vector).T, np.linalg.pinv(covariance_matrix)), (X-mean_vector))/2)\n",
    "    b= (2*np.pi)**(-len(X)/2)*(np.linalg.det(covariance_matrix))**(-1/2)*np.exp(-np.dot(np.dot((X-mean_vector).T, np.linalg.inv(covariance_matrix)), (X-mean_vector))/2)\n",
    "     # c= ((1/(((2*math.pi)**(X.shape[0]/2))*((np.linalg.det(covariance_matrix))**0.5)))*math.exp(-0.5*np.matmul(np.matmul((X-mean_vector).T,np.linalg.pinv(covariance_matrix)),(X-mean_vector))))\n",
    "     # return (2*np.pi)**(-len(X)/2)*np.linalg.det(covariance_matrix)**(-1/2)*np.exp(-np.dot(np.dot((X-mean_vector).T, np.linalg.inv(covariance_matrix)), (X-mean_vector))/2)\n",
    "\n",
    "    return b\n",
    "\n",
    "# The only hyperparameter is k ( no.of components for each class)\n",
    "k=3 \n",
    "train=['coast','forest','opencountry','street','tallbuilding']\n",
    "\n",
    "for c, train_file in enumerate(train):\n",
    "    data=pd.read_csv('dataset/'+train_file+'/train.csv')\n",
    "    data=data.to_numpy()\n",
    "    X=data[:,1:]\n",
    "    size.append(len(X))\n",
    "    print(f\"\\n\\n\\nClass {c}\")\n",
    "#     print(size)\n",
    "    kmeans=KMeans(n_clusters=k,random_state=0).fit(X)\n",
    "    # kmeans=KMeans(n_clusters=k).fit(X)\n",
    "    means_old=kmeans.cluster_centers_\n",
    "    labels=kmeans.labels_\n",
    "\n",
    "\n",
    "    N=len(X)\n",
    "    r_old=np.zeros((len(X),k)) # form a Z ( indicator ) matrix\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        r_old[i,labels[i]]=1\n",
    "\n",
    "    Nq_old=np.sum(r_old,axis=0) # sum conatins the number of elements belonging\n",
    "                                 # to each cluster\n",
    "\n",
    "    print(\"\\nOriginal effective number of elements in each cluster\")\n",
    "    print(Nq_old)\n",
    "    # Initialization\n",
    "\n",
    "    #cov2 is a 3-d array containing the covariance matrix of each cluster\n",
    "    cov_old=np.zeros([k,X.shape[1],X.shape[1]])\n",
    "    Wq_old =np.zeros([k,1]) ## weight of each cluster\n",
    "\n",
    "    for i in range(k):\n",
    "        Nq=Nq_old[i]\n",
    "        Wq_old[i]= Nq/N\n",
    "        tp=np.zeros([X.shape[1],X.shape[1]])\n",
    "\n",
    "        for p in range(X.shape[0]):\n",
    "            le=X[p,:]-means_old[i]\n",
    "            le=np.reshape(le,[le.shape[0],1])\n",
    "            tp=tp+r_old[p,i]*(np.dot(le,le.T))\n",
    "        tp=tp/Nq\n",
    "\n",
    "#         d= np.diag(tp)\n",
    "#         tp=np.diag(d)\n",
    "        cov_old[i,:,:]=tp.copy()\n",
    "\n",
    "    ll_old= 0.0\n",
    "    for n in range(len(X)):\n",
    "        ll_old = ll_old + np.log(sum([Wq_old[j]*multivariate_normal.pdf(X[n], means_old[j], cov_old[j],allow_singular=True) for j in range(k)]))\n",
    "\n",
    "    print(f\"\\nInitial log-likehood = {ll_old}\")\n",
    "\n",
    "    convergence=False\n",
    "    iter_convergence=0\n",
    "    run=0\n",
    "    runs=1000\n",
    "    epsilon=100\n",
    "\n",
    "    while (convergence == False and run<runs):\n",
    "\n",
    "        # ''' --------------------------   E - STEP   -------------------------- '''\n",
    "\n",
    "        # Initiating the r matrix, every row contains the probabilities\n",
    "        # for every cluster for this row\n",
    "\n",
    "        r_new = np.zeros((len(X), k))  # responsibilty matrix\n",
    "\n",
    "        # Calculating the r matrix\n",
    "        for n in range(len(X)):\n",
    "            for i in range(k):\n",
    "                r_new[n][i] = Wq_old[i] * multivariate_normal.pdf(X[n], means_old[i], cov_old[i],allow_singular=True)\n",
    "                r_new[n][i] /= sum([Wq_old[j]*multivariate_normal.pdf(X[n], means_old[j], cov_old[j],allow_singular=True) for j in range(k)])\n",
    "\n",
    "        # Calculating the N effective elemts fro each component\n",
    "        Nq_new = np.sum(r_new, axis=0)\n",
    "\n",
    "\n",
    "        # ''' --------------------------   M - STEP   -------------------------- '''\n",
    "\n",
    "\n",
    "        # Updating the weights list\n",
    "        Wq_new =np.zeros([k,1]) ## weight of each cluster\n",
    "        for i in range(k):\n",
    "            Wq_new[i]= Nq_new[i]/ N\n",
    "\n",
    "\n",
    "        # Initializing the mean vector as a zero vector\n",
    "        means_new = np.zeros((k, len(X[0])))\n",
    "\n",
    "        # Updating the mean vector\n",
    "        for i in range(k):\n",
    "            for n in range(len(X)):\n",
    "                means_new[i] = means_new[i] + r_new[n][i] * X[n]\n",
    "            means_new[i] = means_new [i]/Nq_new[i]\n",
    "\n",
    "\n",
    "\n",
    "        # Initiating the list of the covariance matrixes\n",
    "        cov_new =np.zeros([k,X.shape[1],X.shape[1]])\n",
    "\n",
    "        # Updating the covariance matrices\n",
    "        for i in range(k):\n",
    "            Nq=Nq_new[i]\n",
    "            tp=np.zeros([X.shape[1],X.shape[1]])\n",
    "\n",
    "            for p in range(X.shape[0]):\n",
    "                le=X[p,:]-means_new[i]\n",
    "                le=np.reshape(le,[le.shape[0],1])\n",
    "                tp=tp+r_new[p,i]*(np.dot(le,le.T))\n",
    "\n",
    "            tp=tp/Nq\n",
    "#             d= np.diag(tp)\n",
    "#             tp=np.diag(d)\n",
    "            cov_new[i,:,:]=tp.copy()\n",
    "\n",
    "\n",
    "        # print(f\"\\nRun= {run}\\n\")\n",
    "#         print(np.sum(Nq_new))\n",
    "#         print(\"\\nWeights\\n\")\n",
    "#         print(np.sum(Wq_new))\n",
    "#         print(Wq_new)\n",
    "#         print(np.sum(r_new))\n",
    "#         print(\"\\n------------------\")\n",
    "\n",
    "        # Calculating log-likelhood\n",
    "        ll_new=0\n",
    "        for n in range(len(X)):\n",
    "            ll_new = ll_new + np.log(sum([Wq_new[j]*multivariate_normal.pdf(X[n], means_new[j], cov_new[j],allow_singular=True) for j in range(k)]))\n",
    "\n",
    "    #     print(ll_new)\n",
    "        diff=ll_new-ll_old\n",
    "\n",
    "    #     print(diff)\n",
    "\n",
    "        #Convergence condition\n",
    "        if diff < 1e-3:\n",
    "            iter_convergence=run\n",
    "            convergence=True\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            ll_old=ll_new.copy()\n",
    "            Wq_old= Wq_new.copy()\n",
    "            means_old=means_new.copy()\n",
    "            cov_old=cov_new.copy()\n",
    "\n",
    "        run= run +1\n",
    "\n",
    "    if convergence==True and run!=runs:\n",
    "        print(\"Iterations for convergence=\",iter_convergence)\n",
    "    else:\n",
    "        print(\"Estimate has not converged yet, more runs needed\")\n",
    "    print(f\"Final log-likehood = {ll_new}\")\n",
    "\n",
    "    print(\"\\nEffective number of elements in each cluster is\")\n",
    "    print(Nq_new)\n",
    "#     ass=np.sum(Nq_new)\n",
    "#     print(ass)\n",
    "    weights.append(Wq_new)\n",
    "    means.append(means_new)\n",
    "    cov.append(cov_new)\n",
    "\n",
    "print(\"\\n##############################################################################\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    image_names         0         1         2         3  \\\n",
      "0              coast_nat904.jpg  0.000000  0.007507  0.235611  0.169693   \n",
      "1              coast_natu25.jpg  0.312500  0.028046  0.033295  0.118454   \n",
      "2             coast_natu791.jpg  0.750504  0.093353  0.037109  0.030624   \n",
      "3              coast_nat893.jpg  0.064850  0.189560  0.146851  0.211685   \n",
      "4             coast_natu619.jpg  0.002441  0.135590  0.419647  0.273682   \n",
      "..                          ...       ...       ...       ...       ...   \n",
      "175      tallbuilding_city8.jpg  0.287766  0.158173  0.064896  0.065506   \n",
      "176     tallbuilding_urb974.jpg  0.475754  0.189987  0.072449  0.080658   \n",
      "177  tallbuilding_urban1158.jpg  0.162399  0.419479  0.139389  0.079437   \n",
      "178     tallbuilding_urb390.jpg  0.178162  0.076935  0.165024  0.132324   \n",
      "179  tallbuilding_urban1126.jpg  0.109070  0.281845  0.278763  0.102005   \n",
      "\n",
      "            4         5         6         7         8  ...        15  \\\n",
      "0    0.104401  0.096832  0.163010  0.222946  0.000000  ...  0.000000   \n",
      "1    0.123184  0.141037  0.123123  0.120361  0.330658  ...  0.025635   \n",
      "2    0.023422  0.023346  0.023331  0.018311  0.496582  ...  0.018799   \n",
      "3    0.162338  0.124176  0.100235  0.000305  0.007980  ...  0.100021   \n",
      "4    0.089478  0.033829  0.023468  0.021866  0.004913  ...  0.025314   \n",
      "..        ...       ...       ...       ...       ...  ...       ...   \n",
      "175  0.145493  0.118744  0.095810  0.063614  0.271805  ...  0.031906   \n",
      "176  0.054382  0.043640  0.033310  0.049820  0.466965  ...  0.014862   \n",
      "177  0.186325  0.007294  0.003860  0.001816  0.097198  ...  0.000168   \n",
      "178  0.027222  0.163559  0.138184  0.118591  0.188950  ...  0.058533   \n",
      "179  0.060410  0.077591  0.070633  0.019684  0.082169  ...  0.010651   \n",
      "\n",
      "           16        17        18        19        20        21        22  \\\n",
      "0    0.004272  0.934540  0.061188  0.000000  0.000000  0.000000  0.000000   \n",
      "1    0.336349  0.012329  0.066742  0.144058  0.244949  0.192459  0.001022   \n",
      "2    0.468964  0.183289  0.149200  0.087265  0.032455  0.020889  0.025055   \n",
      "3    0.008606  0.008057  0.068588  0.102020  0.071274  0.058365  0.082687   \n",
      "4    0.022491  0.157806  0.271622  0.165680  0.058884  0.161530  0.093857   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "175  0.323425  0.166351  0.049561  0.032013  0.215271  0.180481  0.031525   \n",
      "176  0.277191  0.273651  0.238480  0.105392  0.043655  0.041641  0.019348   \n",
      "177  0.058167  0.202927  0.293396  0.394928  0.028610  0.021790  0.000183   \n",
      "178  0.244858  0.111145  0.174133  0.037109  0.057678  0.273682  0.098206   \n",
      "179  0.056961  0.312851  0.324661  0.113068  0.092789  0.077042  0.021286   \n",
      "\n",
      "           23  y  \n",
      "0    0.000000  0  \n",
      "1    0.002090  0  \n",
      "2    0.032883  0  \n",
      "3    0.600403  0  \n",
      "4    0.068130  0  \n",
      "..        ... ..  \n",
      "175  0.001373  4  \n",
      "176  0.000641  4  \n",
      "177  0.000000  4  \n",
      "178  0.003189  4  \n",
      "179  0.001343  4  \n",
      "\n",
      "[180 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "size=np.array(size)\n",
    "prior_class=size/np.sum(size)\n",
    "\n",
    "validation_set=['coast','forest','opencountry','street','tallbuilding']\n",
    "# validation_set=['train_1.csv','train_2.csv','train_3.csv','train_4.csv','train_5.csv']\n",
    "\n",
    "\n",
    "valid_data=pd.DataFrame()\n",
    "test_data=pd.DataFrame()\n",
    "\n",
    "for ind, valid_file in enumerate(validation_set):\n",
    "\n",
    "    X_valid=pd.read_csv('dataset/'+valid_file+'/dev.csv')\n",
    "    X_valid['y']=ind\n",
    "    msk = np.random.rand(len(X_valid)) < 0.5  #50-50 splits\n",
    "    #print(X_valid[msk])\n",
    "    valid_data=pd.concat([valid_data,X_valid[msk]],ignore_index=True)\n",
    "    test_data=pd.concat([test_data,X_valid[~msk]],ignore_index=True)\n",
    "    \n",
    "#print(valid_data)\n",
    "#print(test_data)  \n",
    " \n",
    "print(valid_data)\n",
    "print(test_data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor ind, valid_file in enumerate(validation_set):\\n\\n    \\n    X_valid=X_valid.to_numpy()\\n    X_valid=X_valid[:,1:]\\n\\n    index=[]\\n\\n    for n in range(len(X_valid)):\\n        ll_n=[]\\n        for i in range(len(validation_set)):\\n            ll= np.log(sum([weights[i][j]*multivariate_normal.pdf(X_valid[n], means[i][j], cov[i][j],allow_singular=True)  for j in range(k)])) + np.log(prior_class[i])\\n            ll_n.append(ll)\\n        ll_n=np.array(ll_n)\\n        index.append(np.argmax(ll_n))\\n\\n    p=index.count(ind)\\n    prob=p/len(index)\\n\\n    print(prob)\\n    \\n    '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for ind, valid_file in enumerate(validation_set):\n",
    "\n",
    "    \n",
    "    X_valid=X_valid.to_numpy()\n",
    "    X_valid=X_valid[:,1:]\n",
    "\n",
    "    index=[]\n",
    "\n",
    "    for n in range(len(X_valid)):\n",
    "        ll_n=[]\n",
    "        for i in range(len(validation_set)):\n",
    "            ll= np.log(sum([weights[i][j]*multivariate_normal.pdf(X_valid[n], means[i][j], cov[i][j],allow_singular=True)  for j in range(k)])) + np.log(prior_class[i])\n",
    "            ll_n.append(ll)\n",
    "        ll_n=np.array(ll_n)\n",
    "        index.append(np.argmax(ll_n))\n",
    "\n",
    "    p=index.count(ind)\n",
    "    prob=p/len(index)\n",
    "\n",
    "    print(prob)\n",
    "    \n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-e0e369bfe355>:21: RuntimeWarning: divide by zero encountered in log\n",
      "  ll= np.log(sum([weights[i][j]*multivariate_normal.pdf(X_valid, means[i][j], cov[i][j],allow_singular=True)  for j in range(k)])) + np.log(prior_class[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on validation set using full covariance matrix and k=3 is 65.71428571428571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predicted=[]\n",
    "real=[]\n",
    "k=3\n",
    "#print(len(validation_set))\n",
    "for i in range(len(valid_data)):\n",
    "\n",
    "    X_valid=(valid_data.loc[i,:]).to_numpy()\n",
    "    #print(X_valid)\n",
    "    X_valid=X_valid[1:]\n",
    "    Y_valid=X_valid[-1]\n",
    "    X_valid=X_valid[:-1]\n",
    "    real.append(Y_valid)\n",
    "    \n",
    "\n",
    "    #for n in range(len(X_valid)):\n",
    "    ll_n=[]\n",
    "    for i in range(len(validation_set)):\n",
    "        ll= np.log(sum([weights[i][j]*multivariate_normal.pdf(X_valid, means[i][j], cov[i][j],allow_singular=True)  for j in range(k)])) + np.log(prior_class[i])\n",
    "        ll_n.append(ll)\n",
    "    ll_n=np.array(ll_n)\n",
    "    predicted.append(np.argmax(ll_n))\n",
    "    \n",
    "    #p=index.count(ind)\n",
    "    #prob=p/len(index)\n",
    "\n",
    "    #print(prob)\n",
    "    \n",
    "print(\"accuracy on validation set using full covariance matrix and k=3 is \"+str(accuracy_score(real,predicted)*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
